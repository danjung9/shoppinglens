"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var fallback_adapter_exports = {};
__export(fallback_adapter_exports, {
  FallbackAdapter: () => FallbackAdapter
});
module.exports = __toCommonJS(fallback_adapter_exports);
var import_exceptions = require("../_exceptions.cjs");
var import_log = require("../log.cjs");
var import_types = require("../types.cjs");
var import_llm = require("./llm.cjs");
const DEFAULT_FALLBACK_API_CONNECT_OPTIONS = {
  maxRetry: 0,
  timeoutMs: import_types.DEFAULT_API_CONNECT_OPTIONS.timeoutMs,
  retryIntervalMs: import_types.DEFAULT_API_CONNECT_OPTIONS.retryIntervalMs
};
class FallbackAdapter extends import_llm.LLM {
  llms;
  attemptTimeout;
  maxRetryPerLLM;
  retryInterval;
  retryOnChunkSent;
  /** @internal */
  _status;
  logger = (0, import_log.log)();
  constructor(options) {
    super();
    if (!options.llms || options.llms.length < 1) {
      throw new Error("at least one LLM instance must be provided.");
    }
    this.llms = options.llms;
    this.attemptTimeout = options.attemptTimeout ?? 5;
    this.maxRetryPerLLM = options.maxRetryPerLLM ?? 0;
    this.retryInterval = options.retryInterval ?? 0.5;
    this.retryOnChunkSent = options.retryOnChunkSent ?? false;
    this._status = this.llms.map(() => ({
      available: true,
      recoveringTask: null
    }));
    for (const llm of this.llms) {
      llm.on("metrics_collected", (metrics) => {
        this.emit("metrics_collected", metrics);
      });
    }
  }
  get model() {
    return "FallbackAdapter";
  }
  label() {
    return "FallbackAdapter";
  }
  chat(opts) {
    return new FallbackLLMStream(this, {
      chatCtx: opts.chatCtx,
      toolCtx: opts.toolCtx,
      connOptions: opts.connOptions || DEFAULT_FALLBACK_API_CONNECT_OPTIONS,
      parallelToolCalls: opts.parallelToolCalls,
      toolChoice: opts.toolChoice,
      extraKwargs: opts.extraKwargs
    });
  }
  /**
   * Emit availability changed event.
   * @internal
   */
  _emitAvailabilityChanged(llm, available) {
    const event = { llm, available };
    this.emit(
      "llm_availability_changed",
      event
    );
  }
}
class FallbackLLMStream extends import_llm.LLMStream {
  adapter;
  parallelToolCalls;
  toolChoice;
  extraKwargs;
  _currentStream;
  _log = (0, import_log.log)();
  constructor(adapter, opts) {
    super(adapter, {
      chatCtx: opts.chatCtx,
      toolCtx: opts.toolCtx,
      connOptions: opts.connOptions
    });
    this.adapter = adapter;
    this.parallelToolCalls = opts.parallelToolCalls;
    this.toolChoice = opts.toolChoice;
    this.extraKwargs = opts.extraKwargs;
  }
  /**
   * Override chatCtx to return current stream's context if available.
   */
  get chatCtx() {
    var _a;
    return ((_a = this._currentStream) == null ? void 0 : _a.chatCtx) ?? super.chatCtx;
  }
  /**
   * Try to generate with a single LLM.
   * Returns an async generator that yields chunks.
   */
  async *tryGenerate(llm, checkRecovery = false) {
    const connOptions = {
      ...this.connOptions,
      maxRetry: this.adapter.maxRetryPerLLM,
      timeoutMs: this.adapter.attemptTimeout * 1e3,
      retryIntervalMs: this.adapter.retryInterval * 1e3
    };
    const stream = llm.chat({
      chatCtx: super.chatCtx,
      toolCtx: this.toolCtx,
      connOptions,
      parallelToolCalls: this.parallelToolCalls,
      toolChoice: this.toolChoice,
      extraKwargs: this.extraKwargs
    });
    let streamError;
    const errorHandler = (ev) => {
      streamError = ev.error;
    };
    llm.on("error", errorHandler);
    try {
      let shouldSetCurrent = !checkRecovery;
      for await (const chunk of stream) {
        if (shouldSetCurrent) {
          shouldSetCurrent = false;
          this._currentStream = stream;
        }
        yield chunk;
      }
      if (streamError) {
        throw streamError;
      }
    } catch (error) {
      if (error instanceof import_exceptions.APIError) {
        if (checkRecovery) {
          this._log.warn({ llm: llm.label(), error }, "recovery failed");
        } else {
          this._log.warn({ llm: llm.label(), error }, "failed, switching to next LLM");
        }
        throw error;
      }
      if (error instanceof Error && error.name === "AbortError") {
        if (checkRecovery) {
          this._log.warn({ llm: llm.label() }, "recovery timed out");
        } else {
          this._log.warn({ llm: llm.label() }, "timed out, switching to next LLM");
        }
        throw error;
      }
      if (checkRecovery) {
        this._log.error({ llm: llm.label(), error }, "recovery unexpected error");
      } else {
        this._log.error({ llm: llm.label(), error }, "unexpected error, switching to next LLM");
      }
      throw error;
    } finally {
      llm.off("error", errorHandler);
    }
  }
  /**
   * Start background recovery task for an LLM.
   */
  tryRecovery(llm, index) {
    const status = this.adapter._status[index];
    if (status.recoveringTask !== null) {
      return;
    }
    const recoverTask = async () => {
      try {
        for await (const _chunk of this.tryGenerate(llm, true)) {
        }
        status.available = true;
        this._log.info({ llm: llm.label() }, "LLM recovered");
        this.adapter._emitAvailabilityChanged(llm, true);
      } catch {
      } finally {
        status.recoveringTask = null;
      }
    };
    status.recoveringTask = recoverTask();
  }
  /**
   * Main run method - iterates through LLMs with fallback logic.
   */
  async run() {
    const startTime = Date.now();
    const allFailed = this.adapter._status.every((s) => !s.available);
    if (allFailed) {
      this._log.error("all LLMs are unavailable, retrying...");
    }
    for (let i = 0; i < this.adapter.llms.length; i++) {
      const llm = this.adapter.llms[i];
      const status = this.adapter._status[i];
      this._log.debug(
        { llm: llm.label(), index: i, available: status.available, allFailed },
        "checking LLM"
      );
      if (status.available || allFailed) {
        let textSent = "";
        const toolCallsSent = [];
        try {
          this._log.info({ llm: llm.label() }, "FallbackAdapter: Attempting provider");
          let chunkCount = 0;
          for await (const chunk of this.tryGenerate(llm, false)) {
            chunkCount++;
            if (chunk.delta) {
              if (chunk.delta.content) {
                textSent += chunk.delta.content;
              }
              if (chunk.delta.toolCalls) {
                for (const tc of chunk.delta.toolCalls) {
                  if (tc.name) {
                    toolCallsSent.push(tc.name);
                  }
                }
              }
            }
            this._log.debug({ llm: llm.label(), chunkCount }, "run: forwarding chunk to queue");
            this.queue.put(chunk);
          }
          this._log.info(
            { llm: llm.label(), totalChunks: chunkCount, textLength: textSent.length },
            "FallbackAdapter: Provider succeeded"
          );
          return;
        } catch (error) {
          if (status.available) {
            status.available = false;
            this.adapter._emitAvailabilityChanged(llm, false);
          }
          if (textSent || toolCallsSent.length > 0) {
            const extra = { textSent, toolCallsSent };
            if (!this.adapter.retryOnChunkSent) {
              this._log.error(
                { llm: llm.label(), ...extra },
                "failed after sending chunk, skip retrying. Set `retryOnChunkSent` to `true` to enable."
              );
              throw error;
            }
            this._log.warn(
              { llm: llm.label(), ...extra },
              "failed after sending chunk, retrying..."
            );
          }
        }
      }
      this.tryRecovery(llm, i);
    }
    const duration = (Date.now() - startTime) / 1e3;
    const labels = this.adapter.llms.map((l) => l.label()).join(", ");
    throw new import_exceptions.APIConnectionError({
      message: `all LLMs failed (${labels}) after ${duration.toFixed(2)}s`
    });
  }
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  FallbackAdapter
});
//# sourceMappingURL=fallback_adapter.cjs.map