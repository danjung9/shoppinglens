import ffmpegInstaller from "@ffmpeg-installer/ffmpeg";
import { Mutex } from "@livekit/mutex";
import { AudioFrame, AudioResampler } from "@livekit/rtc-node";
import ffmpeg from "fluent-ffmpeg";
import fs from "node:fs";
import path from "node:path";
import { PassThrough } from "node:stream";
import { TransformStream } from "node:stream/web";
import { log } from "../../log.js";
import { isStreamReaderReleaseError } from "../../stream/deferred_stream.js";
import { createStreamChannel } from "../../stream/stream_channel.js";
import { Future, Task, cancelAndWait, delay } from "../../utils.js";
import { AudioInput, AudioOutput } from "../io.js";
ffmpeg.setFfmpegPath(ffmpegInstaller.path);
const WRITE_INTERVAL_MS = 2500;
const DEFAULT_SAMPLE_RATE = 48e3;
class RecorderIO {
  inRecord;
  outRecord;
  inChan = createStreamChannel();
  outChan = createStreamChannel();
  session;
  sampleRate;
  _outputPath;
  forwardTask;
  encodeTask;
  closeFuture = new Future();
  lock = new Mutex();
  started = false;
  // FFmpeg streaming state
  pcmStream;
  ffmpegPromise;
  inResampler;
  outResampler;
  logger = log();
  constructor(opts) {
    const { agentSession, sampleRate = DEFAULT_SAMPLE_RATE } = opts;
    this.session = agentSession;
    this.sampleRate = sampleRate;
  }
  async start(outputPath) {
    const unlock = await this.lock.lock();
    try {
      if (this.started) return;
      if (!this.inRecord || !this.outRecord) {
        throw new Error(
          "RecorderIO not properly initialized: both `recordInput()` and `recordOutput()` must be called before starting the recorder."
        );
      }
      this._outputPath = outputPath;
      this.started = true;
      this.closeFuture = new Future();
      const dir = path.dirname(outputPath);
      if (!fs.existsSync(dir)) {
        fs.mkdirSync(dir, { recursive: true });
      }
      this.forwardTask = Task.from(({ signal }) => this.forward(signal));
      this.encodeTask = Task.from(() => this.encode(), void 0, "recorder_io_encode_task");
    } finally {
      unlock();
    }
  }
  async close() {
    const unlock = await this.lock.lock();
    try {
      if (!this.started) return;
      await this.inChan.close();
      await this.outChan.close();
      await this.closeFuture.await;
      await cancelAndWait([this.forwardTask, this.encodeTask]);
      this.started = false;
    } finally {
      unlock();
    }
  }
  recordInput(audioInput) {
    this.inRecord = new RecorderAudioInput(this, audioInput);
    return this.inRecord;
  }
  recordOutput(audioOutput) {
    this.outRecord = new RecorderAudioOutput(this, audioOutput, (buf) => this.writeCb(buf));
    return this.outRecord;
  }
  writeCb(buf) {
    const inputBuf = this.inRecord.takeBuf();
    this.inChan.write(inputBuf);
    this.outChan.write(buf);
  }
  get recording() {
    return this.started;
  }
  get outputPath() {
    return this._outputPath;
  }
  get recordingStartedAt() {
    return this.session._startedAt;
  }
  /**
   * Forward task: periodically flush input buffer to encoder
   */
  async forward(signal) {
    while (!signal.aborted) {
      try {
        await delay(WRITE_INTERVAL_MS, { signal });
      } catch {
        break;
      }
      if (this.outRecord.hasPendingData) {
        continue;
      }
      const inputBuf = this.inRecord.takeBuf();
      this.inChan.write(inputBuf).catch((err) => this.logger.error({ err }, "Error writing RecorderIO input buffer"));
      this.outChan.write([]).catch((err) => this.logger.error({ err }, "Error writing RecorderIO output buffer"));
    }
  }
  /**
   * Start FFmpeg process for streaming encoding
   */
  startFFmpeg() {
    if (this.pcmStream) return;
    this.pcmStream = new PassThrough();
    this.ffmpegPromise = new Promise((resolve, reject) => {
      ffmpeg(this.pcmStream).inputFormat("s16le").inputOptions([`-ar ${this.sampleRate}`, "-ac 2"]).audioCodec("libopus").audioChannels(2).audioFrequency(this.sampleRate).format("ogg").output(this._outputPath).on("end", () => {
        this.logger.debug("FFmpeg encoding finished");
        resolve();
      }).on("error", (err) => {
        var _a, _b, _c, _d;
        if (((_a = err.message) == null ? void 0 : _a.includes("Output stream closed")) || ((_b = err.message) == null ? void 0 : _b.includes("received signal 2")) || ((_c = err.message) == null ? void 0 : _c.includes("SIGKILL")) || ((_d = err.message) == null ? void 0 : _d.includes("SIGINT"))) {
          resolve();
        } else {
          this.logger.error({ err }, "FFmpeg encoding error");
          reject(err);
        }
      }).run();
    });
  }
  /**
   * Resample and mix frames to mono Float32
   */
  resampleAndMix(opts) {
    const INV_INT16 = 1 / 32768;
    const { frames, flush = false } = opts;
    let { resampler } = opts;
    if (frames.length === 0 && !flush) {
      return { samples: new Float32Array(0), resampler };
    }
    if (!resampler && frames.length > 0) {
      const firstFrame = frames[0];
      resampler = new AudioResampler(firstFrame.sampleRate, this.sampleRate, firstFrame.channels);
    }
    const resampledFrames = [];
    for (const frame of frames) {
      if (resampler) {
        resampledFrames.push(...resampler.push(frame));
      }
    }
    if (flush && resampler) {
      resampledFrames.push(...resampler.flush());
    }
    const totalSamples = resampledFrames.reduce((acc, frame) => acc + frame.samplesPerChannel, 0);
    const samples = new Float32Array(totalSamples);
    let pos = 0;
    for (const frame of resampledFrames) {
      const data = frame.data;
      const numChannels = frame.channels;
      for (let i = 0; i < frame.samplesPerChannel; i++) {
        let sum = 0;
        for (let ch = 0; ch < numChannels; ch++) {
          sum += data[i * numChannels + ch];
        }
        samples[pos++] = sum / numChannels * INV_INT16;
      }
    }
    return { samples, resampler };
  }
  /**
   * Write PCM chunk to FFmpeg stream
   */
  writePCM(leftSamples, rightSamples) {
    if (!this.pcmStream) {
      this.startFFmpeg();
    }
    if (leftSamples.length !== rightSamples.length) {
      const diff = Math.abs(leftSamples.length - rightSamples.length);
      if (leftSamples.length < rightSamples.length) {
        this.logger.warn(
          `Input is shorter by ${diff} samples; silence has been prepended to align the input channel.`
        );
        const padded = new Float32Array(rightSamples.length);
        padded.set(leftSamples, diff);
        leftSamples = padded;
      } else {
        const padded = new Float32Array(leftSamples.length);
        padded.set(rightSamples, diff);
        rightSamples = padded;
      }
    }
    const maxLen = Math.max(leftSamples.length, rightSamples.length);
    if (maxLen <= 0) return;
    const stereoData = new Int16Array(maxLen * 2);
    for (let i = 0; i < maxLen; i++) {
      stereoData[i * 2] = Math.max(
        -32768,
        Math.min(32767, Math.round((leftSamples[i] ?? 0) * 32768))
      );
      stereoData[i * 2 + 1] = Math.max(
        -32768,
        Math.min(32767, Math.round((rightSamples[i] ?? 0) * 32768))
      );
    }
    this.pcmStream.write(Buffer.from(stereoData.buffer));
  }
  /**
   * Encode task: read from channels, mix to stereo, stream to FFmpeg
   */
  async encode() {
    if (!this._outputPath) return;
    const inReader = this.inChan.stream().getReader();
    const outReader = this.outChan.stream().getReader();
    try {
      while (true) {
        const [inResult, outResult] = await Promise.all([inReader.read(), outReader.read()]);
        if (inResult.done || outResult.done) {
          break;
        }
        const inputBuf = inResult.value;
        const outputBuf = outResult.value;
        const inMixed = this.resampleAndMix({ frames: inputBuf, resampler: this.inResampler });
        this.inResampler = inMixed.resampler;
        const outMixed = this.resampleAndMix({
          frames: outputBuf,
          resampler: this.outResampler,
          flush: outputBuf.length > 0
        });
        this.outResampler = outMixed.resampler;
        this.writePCM(inMixed.samples, outMixed.samples);
      }
      if (this.pcmStream) {
        this.pcmStream.end();
        await this.ffmpegPromise;
      }
    } catch (err) {
      this.logger.error({ err }, "Error in encode task");
    } finally {
      inReader.releaseLock();
      outReader.releaseLock();
      if (!this.closeFuture.done) {
        this.closeFuture.resolve();
      }
    }
  }
}
class RecorderAudioInput extends AudioInput {
  source;
  recorderIO;
  accFrames = [];
  _startedWallTime;
  constructor(recorderIO, source) {
    super();
    this.recorderIO = recorderIO;
    this.source = source;
    this.deferredStream.setSource(this.createInterceptingStream());
  }
  /**
   * Wall-clock time when the first frame was captured
   */
  get startedWallTime() {
    return this._startedWallTime;
  }
  /**
   * Take accumulated frames and clear the buffer
   */
  takeBuf() {
    const frames = this.accFrames;
    this.accFrames = [];
    return frames;
  }
  /**
   * Creates a stream that intercepts frames from the source,
   * accumulates them when recording, and passes them through unchanged.
   */
  createInterceptingStream() {
    const sourceStream = this.source.stream;
    const reader = sourceStream.getReader();
    const transform = new TransformStream({
      transform: (frame, controller) => {
        if (this.recorderIO.recording) {
          if (this._startedWallTime === void 0) {
            this._startedWallTime = Date.now();
          }
          this.accFrames.push(frame);
        }
        controller.enqueue(frame);
      }
    });
    const pump = async () => {
      const writer = transform.writable.getWriter();
      let sourceError;
      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;
          await writer.write(value);
        }
      } catch (e) {
        if (isStreamReaderReleaseError(e)) return;
        sourceError = e;
      } finally {
        if (sourceError) {
          writer.abort(sourceError);
          return;
        }
        writer.releaseLock();
        try {
          await transform.writable.close();
        } catch {
        }
      }
    };
    pump();
    return transform.readable;
  }
  onAttached() {
    this.source.onAttached();
  }
  onDetached() {
    this.source.onDetached();
  }
}
class RecorderAudioOutput extends AudioOutput {
  recorderIO;
  writeFn;
  accFrames = [];
  _startedWallTime;
  // Pause tracking
  currentPauseStart;
  pauseWallTimes = [];
  // [start, end] pairs
  constructor(recorderIO, audioOutput, writeFn) {
    super(audioOutput.sampleRate, audioOutput, { pause: true });
    this.recorderIO = recorderIO;
    this.writeFn = writeFn;
  }
  get startedWallTime() {
    return this._startedWallTime;
  }
  get hasPendingData() {
    return this.accFrames.length > 0;
  }
  pause() {
    if (this.currentPauseStart === void 0 && this.recorderIO.recording) {
      this.currentPauseStart = Date.now();
    }
    if (this.nextInChain) {
      this.nextInChain.pause();
    }
  }
  /**
   * Resume playback and record the pause interval
   */
  resume() {
    if (this.currentPauseStart !== void 0 && this.recorderIO.recording) {
      this.pauseWallTimes.push([this.currentPauseStart, Date.now()]);
      this.currentPauseStart = void 0;
    }
    if (this.nextInChain) {
      this.nextInChain.resume();
    }
  }
  resetPauseState() {
    this.currentPauseStart = void 0;
    this.pauseWallTimes = [];
  }
  onPlaybackFinished(options) {
    const finishTime = Date.now();
    super.onPlaybackFinished(options);
    if (!this.recorderIO.recording) {
      return;
    }
    if (this.currentPauseStart !== void 0) {
      this.pauseWallTimes.push([this.currentPauseStart, finishTime]);
      this.currentPauseStart = void 0;
    }
    if (this.accFrames.length === 0) {
      this.resetPauseState();
      return;
    }
    const playbackPosition = options.playbackPosition;
    const pauseEvents = [];
    if (this.pauseWallTimes.length > 0) {
      const totalPauseDuration = this.pauseWallTimes.reduce(
        (sum, [start, end]) => sum + (end - start),
        0
      );
      const playbackStartTime = finishTime - playbackPosition * 1e3 - totalPauseDuration;
      let accumulatedPause = 0;
      for (const [pauseStart, pauseEnd] of this.pauseWallTimes) {
        let position = (pauseStart - playbackStartTime - accumulatedPause) / 1e3;
        const duration = (pauseEnd - pauseStart) / 1e3;
        position = Math.max(0, Math.min(position, playbackPosition));
        pauseEvents.push([position, duration]);
        accumulatedPause += pauseEnd - pauseStart;
      }
    }
    const buf = [];
    let accDur = 0;
    const sampleRate = this.accFrames[0].sampleRate;
    const numChannels = this.accFrames[0].channels;
    let pauseIdx = 0;
    let shouldBreak = false;
    for (const frame of this.accFrames) {
      let currentFrame = frame;
      const frameDuration = frame.samplesPerChannel / frame.sampleRate;
      if (frameDuration + accDur > playbackPosition) {
        const [left] = splitFrame(currentFrame, playbackPosition - accDur);
        currentFrame = left;
        shouldBreak = true;
      }
      while (pauseIdx < pauseEvents.length && pauseEvents[pauseIdx][0] <= accDur) {
        const [, pauseDur] = pauseEvents[pauseIdx];
        buf.push(createSilenceFrame(pauseDur, sampleRate, numChannels));
        pauseIdx++;
      }
      const currentFrameDuration = currentFrame.samplesPerChannel / currentFrame.sampleRate;
      while (pauseIdx < pauseEvents.length && pauseEvents[pauseIdx][0] < accDur + currentFrameDuration) {
        const [pausePos, pauseDur] = pauseEvents[pauseIdx];
        const [left, right] = splitFrame(currentFrame, pausePos - accDur);
        buf.push(left);
        accDur += left.samplesPerChannel / left.sampleRate;
        buf.push(createSilenceFrame(pauseDur, sampleRate, numChannels));
        currentFrame = right;
        pauseIdx++;
      }
      buf.push(currentFrame);
      accDur += currentFrame.samplesPerChannel / currentFrame.sampleRate;
      if (shouldBreak) {
        break;
      }
    }
    while (pauseIdx < pauseEvents.length) {
      const [pausePos, pauseDur] = pauseEvents[pauseIdx];
      if (pausePos <= playbackPosition) {
        buf.push(createSilenceFrame(pauseDur, sampleRate, numChannels));
      }
      pauseIdx++;
    }
    if (buf.length > 0) {
      this.writeFn(buf);
    }
    this.accFrames = [];
    this.resetPauseState();
  }
  async captureFrame(frame) {
    await super.captureFrame(frame);
    if (this.recorderIO.recording) {
      if (this._startedWallTime === void 0) {
        this._startedWallTime = Date.now();
      }
      this.accFrames.push(frame);
    }
    if (this.nextInChain) {
      await this.nextInChain.captureFrame(frame);
    }
  }
  flush() {
    super.flush();
    if (this.nextInChain) {
      this.nextInChain.flush();
    }
  }
  clearBuffer() {
    if (this.nextInChain) {
      this.nextInChain.clearBuffer();
    }
  }
}
function createSilenceFrame(duration, sampleRate, numChannels) {
  const samples = Math.floor(duration * sampleRate);
  const data = new Int16Array(samples * numChannels);
  return new AudioFrame(data, sampleRate, numChannels, samples);
}
function splitFrame(frame, position) {
  if (position <= 0) {
    const emptyFrame = new AudioFrame(new Int16Array(0), frame.sampleRate, frame.channels, 0);
    return [emptyFrame, frame];
  }
  const frameDuration = frame.samplesPerChannel / frame.sampleRate;
  if (position >= frameDuration) {
    const emptyFrame = new AudioFrame(new Int16Array(0), frame.sampleRate, frame.channels, 0);
    return [frame, emptyFrame];
  }
  const samplesNeeded = Math.floor(position * frame.sampleRate);
  const numChannels = frame.channels;
  const leftData = frame.data.slice(0, samplesNeeded * numChannels);
  const rightData = frame.data.slice(samplesNeeded * numChannels);
  const leftFrame = new AudioFrame(leftData, frame.sampleRate, frame.channels, samplesNeeded);
  const rightFrame = new AudioFrame(
    rightData,
    frame.sampleRate,
    frame.channels,
    frame.samplesPerChannel - samplesNeeded
  );
  return [leftFrame, rightFrame];
}
export {
  RecorderIO
};
//# sourceMappingURL=recorder_io.js.map